{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01fc00-bb08-4ad3-9476-f1eae1796bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    __IPYTHON__\n",
    "    is_notebook = True\n",
    "    print('Notebook mode')\n",
    "except NameError:\n",
    "    is_notebook = False\n",
    "    print('Script mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a4740f-539c-4270-80d1-5af8c556fbfc",
   "metadata": {},
   "source": [
    "# Pip Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce95ab-77e2-4003-909b-1b5236111549",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_notebook:\n",
    "    !pip install boto3 astropy sfdmap progressbar2 GPUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fff225-e870-4b7f-95b0-220564a5e5c0",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be4039-d67e-4613-bb6b-14bcf9fc9803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure a GPU is available\n",
    "import GPUtil\n",
    "print('GPUs:\\n{0}'.format('\\n'.join(['('+str(i+1)+')\\t'+gpu.name for i,gpu in enumerate(GPUtil.getGPUs())])))\n",
    "import tensorflow as tf\n",
    "assert tf.config.list_physical_devices('GPU')[0].device_type == 'GPU', 'GPU is not available!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b7d43-37fd-47d9-92db-576b39e6c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import boto3\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "# random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# local files paths\n",
    "local_home_dir_path = os.path.expanduser(\"~\")\n",
    "local_work_dir_path = os.path.join(local_home_dir_path, 'thesis2')\n",
    "local_code_dir_path = os.path.join(local_work_dir_path , 'code')\n",
    "\n",
    "# S3 file paths\n",
    "endpoint_url = 'https://s3-west.nrp-nautilus.io'\n",
    "bucket_name = 'tau-astro'\n",
    "prefix = 'almogh'\n",
    "s3_work_dir_path = os.path.join(prefix, 'thesis2')\n",
    "s3_data_dir_path = os.path.join(s3_work_dir_path , 'data')\n",
    "s3_models_dir_path = os.path.join(s3_work_dir_path , 'models')\n",
    "s3_final_table_csv_path = os.path.join(s3_data_dir_path, 'SDSS_DR16_all.csv')\n",
    "\n",
    "s3_client = boto3.client(\"s3\", endpoint_url=endpoint_url)\n",
    "\n",
    "# adding code folder to path\n",
    "sys.path.insert(1, local_code_dir_path)\n",
    "from s3 import to_s3_npy, to_s3_pkl, from_s3_npy, from_s3_pkl, to_s3_fig\n",
    "from s3 import log_s3, s3_save_TF_model\n",
    "from NN import DistanceLayer, SiameseModel, DistillationDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d015ad7-5b0a-4e6c-87c3-1df0efd010e1",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d480a-6c2f-4c18-80f2-e5f3026f64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce71579-20f9-4ffe-849b-0bb5a7369382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model paths\n",
    "s3_model_dir_path = os.path.join(s3_models_dir_path, model_name)\n",
    "s3_model_train_dir_path = os.path.join(s3_model_dir_path, 'train')\n",
    "s3_model_test_dir_path = os.path.join(s3_model_dir_path, 'test')\n",
    "# prepare data paths\n",
    "s3_data_model_dir_path = os.path.join(s3_data_dir_path, model_name)\n",
    "s3_data_train_dir_path = os.path.join(s3_data_model_dir_path, 'train')\n",
    "s3_data_test_dir_path = os.path.join(s3_data_model_dir_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01a8a1-f157-461c-83d1-2fb204d00a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dist_mat_path = os.path.join(s3_models_dir_path, 'SmallRF', 'train', 'dist_mat.npy')\n",
    "dist_mat = from_s3_npy(s3_client, bucket_name, dist_mat_path)\n",
    "X_train = from_s3_npy(s3_client, bucket_name, os.path.join(s3_data_train_dir_path, 'spec.npy'))\n",
    "X_test = from_s3_npy(s3_client, bucket_name, os.path.join(s3_data_test_dir_path, 'spec.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7eaa2-4f78-45d3-9e58-4aef8659d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_SmallRF_train = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_dir_path, 'SmallRF', 'train', 'gs.pkl')) # <- This is equal to g_NN\n",
    "gs = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_model_dir_path, 'gs.pkl'))\n",
    "gs_train = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_train_dir_path, 'gs.pkl'))\n",
    "gs_test = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_test_dir_path, 'gs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad42ab-0e73-4891-bc41-26e50411e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_train = np.array([np.where(gs.index == i)[0][0] for i in gs_train.index])\n",
    "I_test = np.array([np.where(gs.index == i)[0][0] for i in gs_test.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15dd32-7ee5-4640-b738-80135cd87c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat_train = dist_mat[I_train,:][:,I_train]\n",
    "dist_mat_test = dist_mat[I_test,:][:,I_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281199a-32d8-4acf-979f-3c40d7d08105",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_notebook:\n",
    "    print('Notebook mode: running on a tiny slice of the data')\n",
    "    X_train = X_train[:100,:]\n",
    "    X_test = X_test[:10,:]\n",
    "    dist_mat_train = dist_mat_train[:100,:][:,:100]\n",
    "    dist_mat_test = dist_mat_test[:10,:][:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068ba31-e817-4ce1-807b-a32a5b963a07",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e247d25a-255e-4d50-b87e-86b3322aa377",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf27f52-4944-418c-b456-62f120096c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e7ff6-3367-428b-a494-eee894f7f4f8",
   "metadata": {},
   "source": [
    "## Embedding Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1bcd2-e608-4301-8a54-07228512c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "encoding_size = 128\n",
    "\n",
    "# input layer\n",
    "x_in = layers.Input(shape=(N_features, 1))\n",
    "\n",
    "# adding the network layers\n",
    "x = x_in\n",
    "x = layers.Conv1D(64, 31, activation=None, padding='same', kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = activations.relu(x)\n",
    "x = layers.AveragePooling1D( 2, padding='same')(x)\n",
    "x = layers.Conv1D(32, 31, activation=None, padding='same', kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = activations.relu(x)\n",
    "x = layers.AveragePooling1D( 2, padding='same')(x)\n",
    "x = layers.Conv1D(16, 31, activation=None, padding='same', kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = activations.relu(x)\n",
    "x = layers.AveragePooling1D( 2, padding='same')(x)\n",
    "x = layers.Conv1D(8, 31, activation=None, padding='same', kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = activations.relu(x)\n",
    "x = layers.AveragePooling1D( 2, padding='same')(x)\n",
    "x = layers.Conv1D(4, 31, activation=None, padding='same', kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = activations.relu(x)\n",
    "x = layers.AveragePooling1D( 2, padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(hidden_size, kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = activations.relu(x)\n",
    "x = layers.Dense(encoding_size, kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = activations.tanh(x)\n",
    "x_out = x\n",
    "\n",
    "# creating the model\n",
    "encoding = Model(x_in, x_out)\n",
    "encoding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8db5d-8893-43d8-b05a-1d80570b7f18",
   "metadata": {},
   "source": [
    "## Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0566a-c249-41ab-9cdd-4ed179f2e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_input = layers.Input(name=\"first_input\", shape=(N_features))\n",
    "second_input = layers.Input(name=\"second_input\", shape=(N_features))\n",
    "\n",
    "first_encoding = encoding(first_input)\n",
    "second_encoding = encoding(second_input)\n",
    "\n",
    "distance = tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(first_encoding - second_encoding), -1),1e-9))\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[first_input, second_input], outputs=distance\n",
    ")\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad27708-d84d-4e4e-8907-05afa7872400",
   "metadata": {},
   "source": [
    "## Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a6fa0-351c-4cec-9517-482a31185c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(siamese_network, dist_loss='L1')\n",
    "siamese_model.compile(optimizer=optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e226e6c-f8ef-4089-b15b-1b3845993309",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381bd5de-c630-4e68-a1ab-80cde447e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DistillationDataGenerator(X_train, dist_mat_train, batch_size=128, shuffle=True, seed=seed, snr_range_db=[6,40], full_epoch=(not is_notebook), norm=True)\n",
    "test_gen = DistillationDataGenerator(X_test, dist_mat_test, batch_size=128, shuffle=True, seed=seed, snr_range_db=[6,40], full_epoch=(not is_notebook), norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44296b-845e-4790-ac86-45239198f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(fig, ax, e, loss_history, val_loss_history):\n",
    "    if ax.lines:\n",
    "        for i,line in enumerate(ax.lines):\n",
    "            line.set_xdata(e)\n",
    "            if (i==1):\n",
    "                line.set_ydata(loss_history)\n",
    "            else:\n",
    "                line.set_ydata(val_loss_history)\n",
    "    else:\n",
    "        ax.plot(e, loss_history, label='training')\n",
    "        ax.plot(e, val_loss_history, label='test')\n",
    "        ax.legend()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877587d-1d92-4d79-8387-1f3a3860bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "sub_epochs = 5\n",
    "N_chunks = int(epochs/sub_epochs)\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "verbosity = 1 if is_notebook else 2\n",
    "\n",
    "# training loop\n",
    "print('Training for {0} full epochs, and stopping for saving every {1} full epochs, for a total of {2} stages.'.format(epochs,sub_epochs, N_chunks))\n",
    "start_time = time.time()\n",
    "for i_chunk in range(N_chunks):\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('epochs {0}-{1}:'.format(i_chunk*sub_epochs+1, (i_chunk+1)*sub_epochs))\n",
    "    print('-------------------------------------')\n",
    "\n",
    "    # train\n",
    "    try:\n",
    "        # for some reason, the first call to fit will throw KeyError...\n",
    "        history = siamese_model.fit(train_gen, epochs=sub_epochs, validation_data=test_gen, verbose=verbosity)\n",
    "    except KeyError:\n",
    "        history = siamese_model.fit(train_gen, epochs=sub_epochs, validation_data=test_gen, verbose=verbosity)\n",
    "    loss_history += history.history['loss']\n",
    "    val_loss_history += history.history['val_loss']\n",
    "    \n",
    "    # create the figures for the loss\n",
    "    loss_fig, loss_ax = plt.subplots(figsize=(15,8))\n",
    "    loss_ax.set_title('Training curve')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.grid()\n",
    "    log_loss_fig, log_loss_ax = plt.subplots(figsize=(15,8))\n",
    "    log_loss_ax.set_title('Training curve (Log Scale)')\n",
    "    log_loss_ax.set_xlabel('epoch')\n",
    "    log_loss_ax.set_ylabel('log(loss)')\n",
    "    log_loss_ax.grid()\n",
    "    log_loss_ax.set_yscale('log')\n",
    "    \n",
    "    # plot the loss\n",
    "    curr_epochs = (i_chunk+1)*sub_epochs\n",
    "    e = np.arange(curr_epochs)+1\n",
    "    \"\"\"\n",
    "    loss_ax.plot(e, loss_history, label='training')\n",
    "    loss_ax.plot(e, val_loss_history, label='test')\n",
    "    loss_ax.legend()\n",
    "    log_loss_ax.plot(e, loss_history, label='training')\n",
    "    log_loss_ax.plot(e, val_loss_history, label='test')\n",
    "    log_loss_ax.legend()\n",
    "    \"\"\"\n",
    "    plot_loss(loss_fig, loss_ax, e, loss_history, val_loss_history)\n",
    "    plot_loss(log_loss_fig, log_loss_ax, e, loss_history, val_loss_history)\n",
    "    plt.show()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_str = 'TOTAL TIME = {0:.3f} hours'.format((end_time - start_time)/3600)\n",
    "    print(time_str)\n",
    "    \n",
    "    # create a sub dir\n",
    "    s3_save_NN_dir_path_sub_epoch = os.path.join(s3_model_train_dir_path, 'after_{0}_epochs'.format((i_chunk+1)*sub_epochs))\n",
    "    # save the figures\n",
    "    to_s3_fig(loss_fig, s3_client, bucket_name, os.path.join(s3_save_NN_dir_path_sub_epoch, 'loss.png'))\n",
    "    to_s3_fig(log_loss_fig, s3_client, bucket_name, os.path.join(s3_save_NN_dir_path_sub_epoch, 'loss.png'))\n",
    "    # save the losses\n",
    "    to_s3_npy(np.array(loss_history), s3_client, bucket_name, os.path.join(s3_save_NN_dir_path_sub_epoch, 'loss.npy'))\n",
    "    to_s3_npy(np.array(val_loss_history), s3_client, bucket_name, os.path.join(s3_save_NN_dir_path_sub_epoch, 'val_loss.npy'))\n",
    "    # get model summary\n",
    "    stringlist = []\n",
    "    encoding.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    encoding_summary = \"\\n\".join(stringlist)\n",
    "    stringlist = []\n",
    "    siamese_network.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    siamese_network_summary = \"\\n\".join(stringlist)\n",
    "    # save log\n",
    "    log_s3(s3_client, bucket_name, s3_model_train_dir_path, 'NN_log.txt',\n",
    "        dist_mat_path = dist_mat_path,\n",
    "        s3_model_train_dir_path = s3_model_train_dir_path,\n",
    "        training_duration = time_str,\n",
    "        encoding_summary = encoding_summary,\n",
    "        siamese_network_summary = siamese_network_summary\n",
    "        )\n",
    "    # save the network\n",
    "    s3_model_path = os.path.join(s3_save_NN_dir_path_sub_epoch, 'model')\n",
    "    s3_save_TF_model(siamese_model, s3_client, bucket_name, s3_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eec060-75b6-478b-9b0a-12c228730bb2",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736b130-de16-4854-a2fe-7e25d2f5822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "batch_size = 128\n",
    "data_gen = DistillationDataGenerator(X_train,  dist_mat_train, batch_size=128, shuffle=False, seed=seed, full_epoch=True, norm=True)\n",
    "Z_NN = siamese_model.predict(data_gen, verbose=verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ae894-429d-4a37-88a7-4a1a8f614a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full distance matrix\n",
    "N = int((-1+np.sqrt(1+8*len(Z_NN)))/2)\n",
    "D_NN = np.zeros(shape=(N,N))\n",
    "D_NN[np.triu_indices(N)] = Z_NN\n",
    "D_NN = D_NN.T\n",
    "D_NN[np.triu_indices(N)] = Z_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6c31c-6767-4663-b388-22d6f404cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the distance matrix\n",
    "to_s3_npy(D_NN, s3_client, bucket_name, os.path.join(s3_save_NN_dir_path_sub_epoch, 'dist_mat.npy'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
