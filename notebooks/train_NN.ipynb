{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e01fc00-bb08-4ad3-9476-f1eae1796bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook mode\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    __IPYTHON__\n",
    "    is_notebook = True\n",
    "    print('Notebook mode')\n",
    "except NameError:\n",
    "    is_notebook = False\n",
    "    print('Script mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f91721-e7de-42ec-9105-b5aad92ad538",
   "metadata": {},
   "outputs": [],
   "source": [
    "IsSmallDataSlice = True\n",
    "IsShortEpochs = False\n",
    "IsShortTraining = True\n",
    "IsSaveModel = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a4740f-539c-4270-80d1-5af8c556fbfc",
   "metadata": {},
   "source": [
    "# Pip Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ce95ab-77e2-4003-909b-1b5236111549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.26.66)\n",
      "Requirement already satisfied: astropy in /opt/conda/lib/python3.10/site-packages (5.2.1)\n",
      "Requirement already satisfied: sfdmap in /opt/conda/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: progressbar2 in /opt/conda/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: GPUtil in /opt/conda/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.66 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.29.66)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /opt/conda/lib/python3.10/site-packages (from astropy) (6.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from astropy) (1.23.5)\n",
      "Requirement already satisfied: packaging>=19.0 in /opt/conda/lib/python3.10/site-packages (from astropy) (21.3)\n",
      "Requirement already satisfied: pyerfa>=2.0 in /opt/conda/lib/python3.10/site-packages (from astropy) (2.0.0.1)\n",
      "Requirement already satisfied: python-utils>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from progressbar2) (3.4.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.66->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.66->boto3) (1.26.11)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=19.0->astropy) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.66->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "if is_notebook:\n",
    "    !pip install boto3 astropy sfdmap progressbar2 GPUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fff225-e870-4b7f-95b0-220564a5e5c0",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "713f36ea-30ba-46fe-8d03-0af1def3827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs:\n",
      "(1)\tNVIDIA GeForce RTX 3090\n",
      "(2)\tNVIDIA GeForce RTX 3090\n",
      "(3)\tNVIDIA GeForce RTX 3090\n",
      "(4)\tNVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "print('GPUs:\\n{0}'.format('\\n'.join(['('+str(i+1)+')\\t'+gpu.name for i,gpu in enumerate(GPUtil.getGPUs())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae34e55-852b-4742-a159-5091ccfe6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GPUs = len(GPUtil.getGPUs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba1126d-04c1-4bf0-a077-f55b74d1f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = GPUtil.getGPUs()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e1fd897-33dd-4e92-a32b-113d7adaa346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24268.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu.memoryFree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b09b910-34cc-41ae-9564-67e9735e56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_notebook:\n",
    "    !export TF_GPU_THREAD_MODE=\"gpu_private\"\n",
    "    if N_GPUs>1:\n",
    "        if N_GPUs==2:\n",
    "            !export TF_MIN_GPU_MULTIPROCESSOR_COUNT=2\n",
    "            !export CUDA_VISIBLE_DEVICES=\"0,1\"\n",
    "        if N_GPUs==3:\n",
    "            !export TF_MIN_GPU_MULTIPROCESSOR_COUNT=3\n",
    "            !export CUDA_VISIBLE_DEVICES=\"0,1,2\"\n",
    "        if N_GPUs==4:\n",
    "            !export TF_MIN_GPU_MULTIPROCESSOR_COUNT=4\n",
    "            !export CUDA_VISIBLE_DEVICES=\"0,1,2,3\"\n",
    "        if N_GPUs==5:\n",
    "            !export TF_MIN_GPU_MULTIPROCESSOR_COUNT=5\n",
    "            !export CUDA_VISIBLE_DEVICES=\"0,1,2,3,4\"\n",
    "        if N_GPUs==6:\n",
    "            !export TF_MIN_GPU_MULTIPROCESSOR_COUNT=6\n",
    "            !export CUDA_VISIBLE_DEVICES=\"0,1,2,3,4,5\"\n",
    "        if N_GPUs==7:\n",
    "            !export TF_MIN_GPU_MULTIPROCESSOR_COUNT=7\n",
    "            !export CUDA_VISIBLE_DEVICES=\"0,1,2,3,4,5,6\"\n",
    "        if N_GPUs==8:\n",
    "            !export TF_MIN_GPU_MULTIPROCESSOR_COUNT=8\n",
    "            !export CUDA_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62adc53d-40f3-405f-9dc1-1233ce6feab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if N_GPUs>1:\n",
    "    os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"]=str(N_GPUs)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=','.join([str(i) for i in range(N_GPUs)])\n",
    "os.environ[\"TF_GPU_THREAD_MODE\"]=\"gpu_private\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47be4039-d67e-4613-bb6b-14bcf9fc9803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 20:48:04.609626: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 20:48:05.203463: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-07 20:48:05.203510: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-07 20:48:05.203516: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "assert len(tf.config.list_physical_devices('GPU'))==N_GPUs, 'Not all GPUs are available!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f7eada4-1aa9-420a-aa90-fbdcb0ccce38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c2b7d43-37fd-47d9-92db-576b39e6c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import boto3\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "# random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# local files paths\n",
    "local_home_dir_path = os.path.expanduser(\"~\")\n",
    "local_work_dir_path = os.path.join(local_home_dir_path, 'thesis2')\n",
    "local_code_dir_path = os.path.join(local_work_dir_path , 'code')\n",
    "\n",
    "# S3 file paths\n",
    "endpoint_url = 'https://s3-west.nrp-nautilus.io'\n",
    "bucket_name = 'tau-astro'\n",
    "prefix = 'almogh'\n",
    "s3_work_dir_path = os.path.join(prefix, 'thesis2')\n",
    "s3_data_dir_path = os.path.join(s3_work_dir_path , 'data')\n",
    "s3_models_dir_path = os.path.join(s3_work_dir_path , 'models')\n",
    "s3_final_table_csv_path = os.path.join(s3_data_dir_path, 'SDSS_DR16_all.csv')\n",
    "\n",
    "s3_client = boto3.client(\"s3\", endpoint_url=endpoint_url)\n",
    "\n",
    "# adding code folder to path\n",
    "sys.path.insert(1, local_code_dir_path)\n",
    "from s3 import to_s3_npy, to_s3_pkl, from_s3_npy, from_s3_pkl, to_s3_fig\n",
    "from s3 import log_s3, s3_save_TF_model\n",
    "from NN import DistanceLayer, SiameseModel, DistillationDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d015ad7-5b0a-4e6c-87c3-1df0efd010e1",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "807d480a-6c2f-4c18-80f2-e5f3026f64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_name = 'NN'\n",
    "save_model_name = 'NN3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dce71579-20f9-4ffe-849b-0bb5a7369382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model paths\n",
    "s3_model_dir_path = os.path.join(s3_models_dir_path, save_model_name)\n",
    "s3_model_train_dir_path = os.path.join(s3_model_dir_path, 'train')\n",
    "s3_model_test_dir_path = os.path.join(s3_model_dir_path, 'test')\n",
    "# prepare data paths\n",
    "s3_data_model_dir_path = os.path.join(s3_data_dir_path, data_model_name)\n",
    "s3_data_train_dir_path = os.path.join(s3_data_model_dir_path, 'train')\n",
    "s3_data_val_dir_path = os.path.join(s3_data_model_dir_path, 'val')\n",
    "s3_data_test_dir_path = os.path.join(s3_data_dir_path, 'SmallRF', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb01a8a1-f157-461c-83d1-2fb204d00a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from uri: s3://tau-astro/almogh/thesis2/models/SmallRF/train/dist_mat.npy\n",
      "loading from uri: s3://tau-astro/almogh/thesis2/data/NN/train/spec.npy\n",
      "loading from uri: s3://tau-astro/almogh/thesis2/data/NN/val/spec.npy\n",
      "loading from uri: s3://tau-astro/almogh/thesis2/data/SmallRF/test/spec.npy\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dist_mat_path = os.path.join(s3_models_dir_path, 'SmallRF', 'train', 'dist_mat.npy')\n",
    "dist_mat = from_s3_npy(s3_client, bucket_name, dist_mat_path)\n",
    "X_train = from_s3_npy(s3_client, bucket_name, os.path.join(s3_data_train_dir_path, 'spec.npy'))\n",
    "X_val = from_s3_npy(s3_client, bucket_name, os.path.join(s3_data_val_dir_path, 'spec.npy'))\n",
    "X_test = from_s3_npy(s3_client, bucket_name, os.path.join(s3_data_test_dir_path, 'spec.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbb7eaa2-4f78-45d3-9e58-4aef8659d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from uri: s3://tau-astro/almogh/thesis2/data/NN/gs.pkl\n",
      "loading from uri: s3://tau-astro/almogh/thesis2/data/NN/train/gs.pkl\n",
      "loading from uri: s3://tau-astro/almogh/thesis2/data/NN/val/gs.pkl\n",
      "loading from uri: s3://tau-astro/almogh/thesis2/data/SmallRF/test/gs.pkl\n"
     ]
    }
   ],
   "source": [
    "# gs_SmallRF_train = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_dir_path, 'SmallRF', 'train', 'gs.pkl')) # <- This is equal to g_NN\n",
    "gs = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_model_dir_path, 'gs.pkl'))\n",
    "gs_train = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_train_dir_path, 'gs.pkl'))\n",
    "gs_val = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_val_dir_path, 'gs.pkl'))\n",
    "gs_test = from_s3_pkl(s3_client, bucket_name, os.path.join(s3_data_test_dir_path, 'gs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bad42ab-0e73-4891-bc41-26e50411e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_train = np.array([np.where(gs.index == i)[0][0] for i in gs_train.index])\n",
    "I_val = np.array([np.where(gs.index == i)[0][0] for i in gs_val.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d15dd32-7ee5-4640-b738-80135cd87c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat_train = dist_mat[I_train,:][:,I_train]\n",
    "dist_mat_val = dist_mat[I_val,:][:,I_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4281199a-32d8-4acf-979f-3c40d7d08105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on a tiny slice of the data\n"
     ]
    }
   ],
   "source": [
    "if IsSmallDataSlice:\n",
    "    print('Running on a tiny slice of the data')\n",
    "    N_nb = 500\n",
    "    X_train = X_train[:N_nb,:]\n",
    "    X_val = X_val[:N_nb,:]\n",
    "    X_test = X_test[:N_nb,:]\n",
    "    dist_mat_train = dist_mat_train[:N_nb,:][:,:N_nb]\n",
    "    dist_mat_val = dist_mat_val[:N_nb,:][:,:N_nb]\n",
    "    gs_train = gs_train[:N_nb]\n",
    "    gs_val = gs_val[:N_nb]\n",
    "    gs_test = gs_test[:N_nb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068ba31-e817-4ce1-807b-a32a5b963a07",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e247d25a-255e-4d50-b87e-86b3322aa377",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b02f9bb-3f08-43da-849a-511a3f6f03cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n",
      "Compute dtype: float16\n",
      "Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bf27f52-4944-418c-b456-62f120096c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from NN import DistanceLayer\n",
    "\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e60991b0-352e-488a-8048-ffedc5da09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "encoding_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e97253f5-1b3c-4901-9ab4-1a56ea14faa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 20:48:17.967879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 20:48:19.352346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22294 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:03:00.0, compute capability: 8.6\n",
      "2023-02-07 20:48:19.352948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22294 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:87:00.0, compute capability: 8.6\n",
      "2023-02-07 20:48:19.353472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22294 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:88:00.0, compute capability: 8.6\n",
      "2023-02-07 20:48:19.354081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22294 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c4:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3'), communication = CommunicationImplementation.AUTO\n"
     ]
    }
   ],
   "source": [
    "if N_GPUs>1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdc1bcd2-e608-4301-8a54-07228512c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    ##############################\n",
    "    #     Embedding Network      #\n",
    "    ##############################\n",
    "    \n",
    "    # input layer\n",
    "    x_in = layers.Input(shape=(N_features, 1))\n",
    "\n",
    "    # adding the network layers\n",
    "    x = x_in\n",
    "    x = layers.Conv1D(64, 31, activation=None, padding='same', kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activations.relu(x)\n",
    "    x = layers.AveragePooling1D( 2, padding='same')(x)\n",
    "    x = layers.Conv1D(32, 31, activation=None, padding='same', kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activations.relu(x)\n",
    "    x = layers.AveragePooling1D( 2, padding='same')(x)\n",
    "    x = layers.Conv1D(16, 31, activation=None, padding='same', kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activations.relu(x)\n",
    "    x = layers.AveragePooling1D( 2, padding='same')(x)\n",
    "    x = layers.Conv1D(8, 31, activation=None, padding='same', kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activations.relu(x)\n",
    "    x = layers.AveragePooling1D( 2, padding='same')(x)\n",
    "    x = layers.Conv1D(4, 31, activation=None, padding='same', kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activations.relu(x)\n",
    "    x = layers.AveragePooling1D( 2, padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(hidden_size, kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = activations.relu(x)\n",
    "    x = layers.Dense(encoding_size, kernel_initializer=initializers.GlorotUniform(seed=seed))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    #x = activations.tanh(x, dtype='float32')\n",
    "    x = layers.Activation('tanh', dtype='float32', name='encoding')(x)\n",
    "    x_out = x\n",
    "\n",
    "    # creating the model\n",
    "    encoding = Model(x_in, x_out)\n",
    "    \n",
    "    ##############################\n",
    "    #     Siamese Network        #\n",
    "    ##############################\n",
    "    first_input = layers.Input(name=\"first_input\", shape=(N_features))\n",
    "    second_input = layers.Input(name=\"second_input\", shape=(N_features))\n",
    "\n",
    "    first_encoding = encoding(first_input)\n",
    "    second_encoding = encoding(second_input)\n",
    "\n",
    "    #distance = tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(first_encoding - second_encoding), -1),1e-9))\n",
    "    distance = DistanceLayer(dtype=mixed_precision.Policy('float32'))(first_encoding, second_encoding)\n",
    "\n",
    "    siamese_network = Model(\n",
    "        inputs=[first_input, second_input], outputs=distance\n",
    "    )\n",
    "    \n",
    "    ##############################\n",
    "    #     Siamese Model          #\n",
    "    ##############################\n",
    "    \n",
    "    siamese_model = SiameseModel(siamese_network, dist_loss='L1')\n",
    "\n",
    "optimizer = optimizers.Adam(0.001)\n",
    "optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "    \n",
    "siamese_model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edb0566a-c249-41ab-9cdd-4ed179f2e4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8400, 1)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 8400, 64)          2048      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 8400, 64)         256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " tf.nn.relu (TFOpLambda)     (None, 8400, 64)          0         \n",
      "                                                                 \n",
      " average_pooling1d (AverageP  (None, 4200, 64)         0         \n",
      " ooling1D)                                                       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 4200, 32)          63520     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 4200, 32)         128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " tf.nn.relu_1 (TFOpLambda)   (None, 4200, 32)          0         \n",
      "                                                                 \n",
      " average_pooling1d_1 (Averag  (None, 2100, 32)         0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 2100, 16)          15888     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 2100, 16)         64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " tf.nn.relu_2 (TFOpLambda)   (None, 2100, 16)          0         \n",
      "                                                                 \n",
      " average_pooling1d_2 (Averag  (None, 1050, 16)         0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 1050, 8)           3976      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1050, 8)          32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " tf.nn.relu_3 (TFOpLambda)   (None, 1050, 8)           0         \n",
      "                                                                 \n",
      " average_pooling1d_3 (Averag  (None, 525, 8)           0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 525, 4)            996       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 525, 4)           16        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " tf.nn.relu_4 (TFOpLambda)   (None, 525, 4)            0         \n",
      "                                                                 \n",
      " average_pooling1d_4 (Averag  (None, 263, 4)           0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1052)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               539136    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " tf.nn.relu_5 (TFOpLambda)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " encoding (Activation)       (None, 128)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 694,284\n",
      "Trainable params: 692,756\n",
      "Non-trainable params: 1,528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37bff438-5ad0-4ca3-9769-3efd08b7b1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " first_input (InputLayer)       [(None, 8400)]       0           []                               \n",
      "                                                                                                  \n",
      " second_input (InputLayer)      [(None, 8400)]       0           []                               \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 128)          694284      ['first_input[0][0]',            \n",
      "                                                                  'second_input[0][0]']           \n",
      "                                                                                                  \n",
      " distance_layer (DistanceLayer)  (None,)             0           ['model[0][0]',                  \n",
      "                                                                  'model[1][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 694,284\n",
      "Trainable params: 692,756\n",
      "Non-trainable params: 1,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e226e6c-f8ef-4089-b15b-1b3845993309",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "381bd5de-c630-4e68-a1ab-80cde447e26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full epochs\n",
      "DataGenerator initialized with:\n",
      "    X shape = 500x8400\n",
      "    D shape = 500x500\n",
      "    batch_size = 512\n",
      "    shuffle = True\n",
      "    full_epoch = True\n",
      "    norm = True\n",
      "    noise = True\n",
      "    snr_range_db = [6, 40]\n",
      "DataGenerator initialized with:\n",
      "    X shape = 500x8400\n",
      "    D shape = 500x500\n",
      "    batch_size = 512\n",
      "    shuffle = True\n",
      "    full_epoch = True\n",
      "    norm = True\n",
      "    noise = True\n",
      "    snr_range_db = [6, 40]\n"
     ]
    }
   ],
   "source": [
    "if IsShortEpochs:\n",
    "    print('Running short epochs')\n",
    "    full_epoch = False\n",
    "else:\n",
    "    print('Running full epochs')\n",
    "    full_epoch = True\n",
    "    \n",
    "batch_size = 128*N_GPUs\n",
    "\n",
    "train_gen = DistillationDataGenerator(X_train, dist_mat_train, batch_size=batch_size, shuffle=True, seed=seed, snr_range_db=[6,40], full_epoch=full_epoch, norm=True)\n",
    "val_gen = DistillationDataGenerator(X_val, dist_mat_val, batch_size=batch_size, shuffle=True, seed=seed, snr_range_db=[6,40], full_epoch=full_epoch, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d44296b-845e-4790-ac86-45239198f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(fig, ax, e, loss_history, val_loss_history):\n",
    "    if ax.lines:\n",
    "        for i,line in enumerate(ax.lines):\n",
    "            line.set_xdata(e)\n",
    "            if (i==1):\n",
    "                line.set_ydata(loss_history)\n",
    "            else:\n",
    "                line.set_ydata(val_loss_history)\n",
    "    else:\n",
    "        ax.plot(e, loss_history, label='training')\n",
    "        ax.plot(e, val_loss_history, label='validation')\n",
    "        ax.legend()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b577595-26f9-4a91-a037-6bea87f8379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile_batch=122,142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 20:48:23.525288: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-02-07 20:48:23.525329: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-02-07 20:48:23.525373: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1664] Profiler found 4 GPUs\n",
      "2023-02-07 20:48:23.525806: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-07 20:48:23.936383: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-02-07 20:48:23.936497: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorBoard callback\n",
    "from datetime import datetime\n",
    "logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "profile_batch = '{0},{1}'.format(str(int(len(train_gen)/2)),str(20+int(len(train_gen)/2)))\n",
    "print('profile_batch={0}'.format(profile_batch))\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = profile_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877587d-1d92-4d79-8387-1f3a3860bb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 full epochs, and stopping for saving every 5 full epochs, for a total of 1 stages.\n",
      "-------------------------------------\n",
      "epochs 1-5:\n",
      "-------------------------------------\n",
      "WARNING:tensorflow:Using CollectiveAllReduceStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 20:48:25.005514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2023-02-07 20:48:27.194243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2023-02-07 20:48:29.242956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2023-02-07 20:48:31.310671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2023-02-07 20:48:32.698338: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\017TensorDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "INFO:tensorflow:Collective all_reduce tensors: 28 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 28 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.AUTO, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 20:48:50.779436: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x557e37a4ab40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-02-07 20:48:50.779487: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-02-07 20:48:50.779501: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-02-07 20:48:50.779512: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-02-07 20:48:50.779521: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-02-07 20:48:50.786131: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-02-07 20:48:50.878471: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "if IsShortTraining:\n",
    "    epochs = 5\n",
    "    sub_epochs = 5\n",
    "else:\n",
    "    epochs = 50\n",
    "    sub_epochs = 5\n",
    "N_chunks = int(epochs/sub_epochs)\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "verbosity = 1 if is_notebook else 2\n",
    "\n",
    "# training loop\n",
    "print('Training for {0} full epochs, and stopping for saving every {1} full epochs, for a total of {2} stages.'.format(epochs,sub_epochs, N_chunks))\n",
    "start_time = time.time()\n",
    "for i_chunk in range(N_chunks):\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('epochs {0}-{1}:'.format(i_chunk*sub_epochs+1, (i_chunk+1)*sub_epochs))\n",
    "    print('-------------------------------------')\n",
    "\n",
    "    # train\n",
    "    try:\n",
    "        # for some reason, the first call to fit will throw KeyError...\n",
    "        history = siamese_model.fit(train_gen, epochs=sub_epochs, validation_data=val_gen, verbose=verbosity, callbacks = [tboard_callback], workers=N_GPUs, use_multiprocessing=True)\n",
    "        #history = siamese_model.fit(train_gen, epochs=sub_epochs, validation_data=val_gen, verbose=verbosity, callbacks = [tboard_callback])\n",
    "    except KeyError:\n",
    "        history = siamese_model.fit(train_gen, epochs=sub_epochs, validation_data=val_gen, verbose=verbosity, callbacks = [tboard_callback], workers=N_GPUs, use_multiprocessing=True)\n",
    "        #history = siamese_model.fit(train_gen, epochs=sub_epochs, validation_data=val_gen, verbose=verbosity, callbacks = [tboard_callback])\n",
    "    loss_history += history.history['loss']\n",
    "    val_loss_history += history.history['val_loss']\n",
    "    \n",
    "    # create the figures for the loss\n",
    "    loss_fig, loss_ax = plt.subplots(figsize=(15,8))\n",
    "    loss_ax.set_title('Training curve')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.grid()\n",
    "    log_loss_fig, log_loss_ax = plt.subplots(figsize=(15,8))\n",
    "    log_loss_ax.set_title('Training curve (Log Scale)')\n",
    "    log_loss_ax.set_xlabel('epoch')\n",
    "    log_loss_ax.set_ylabel('log(loss)')\n",
    "    log_loss_ax.grid()\n",
    "    log_loss_ax.set_yscale('log')\n",
    "    \n",
    "    # plot the loss\n",
    "    curr_epochs = (i_chunk+1)*sub_epochs\n",
    "    e = np.arange(curr_epochs)+1\n",
    "    plot_loss(loss_fig, loss_ax, e, loss_history, val_loss_history)\n",
    "    plot_loss(log_loss_fig, log_loss_ax, e, loss_history, val_loss_history)\n",
    "    plt.show()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_str = 'TOTAL TIME = {0:.3f} hours'.format((end_time - start_time)/3600)\n",
    "    print(time_str)\n",
    "    \n",
    "    if IsSaveModel:\n",
    "    \n",
    "        # create a sub dir\n",
    "        s3_save_NN_dir_path_sub_epoch = os.path.join(s3_model_train_dir_path, 'after_{0}_epochs'.format((i_chunk+1)*sub_epochs))\n",
    "        # save the figures\n",
    "        to_s3_fig(loss_fig, s3_client, bucket_name, os.path.join(s3_save_NN_dir_path_sub_epoch, 'loss.png'))\n",
    "        to_s3_fig(log_loss_fig, s3_client, bucket_name, os.path.join(s3_save_NN_dir_path_sub_epoch, 'loss.png'))\n",
    "        # save the losses\n",
    "        to_s3_npy(np.array(loss_history), s3_client, bucket_name, os.path.join(s3_save_NN_dir_path_sub_epoch, 'loss.npy'))\n",
    "        to_s3_npy(np.array(val_loss_history), s3_client, bucket_name, os.path.join(s3_save_NN_dir_path_sub_epoch, 'val_loss.npy'))\n",
    "        # get model summary\n",
    "        stringlist = []\n",
    "        encoding.summary(print_fn=lambda x: stringlist.append(x))\n",
    "        encoding_summary = \"\\n\".join(stringlist)\n",
    "        stringlist = []\n",
    "        siamese_network.summary(print_fn=lambda x: stringlist.append(x))\n",
    "        siamese_network_summary = \"\\n\".join(stringlist)\n",
    "        # save log\n",
    "        log_s3(s3_client, bucket_name, s3_model_train_dir_path, 'NN_log.txt',\n",
    "            dist_mat_path = dist_mat_path,\n",
    "            s3_model_train_dir_path = s3_model_train_dir_path,\n",
    "            training_duration = time_str,\n",
    "            encoding_summary = encoding_summary,\n",
    "            siamese_network_summary = siamese_network_summary\n",
    "            )\n",
    "        # save the network\n",
    "        s3_model_path = os.path.join(s3_save_NN_dir_path_sub_epoch, 'model')\n",
    "        s3_save_TF_model(siamese_model, s3_client, bucket_name, s3_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eec060-75b6-478b-9b0a-12c228730bb2",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c1793-fe7b-4a65-a087-09bc17d7c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_dist_mat(model, X, verbosity):\n",
    "    # predict\n",
    "    data_gen = DistillationDataGenerator(X, np.zeros(shape=(X.shape[0], X.shape[0])), batch_size=128, shuffle=False, seed=seed, full_epoch=True, norm=True)\n",
    "    Z_NN = model.predict(data_gen, verbose=verbosity)\n",
    "    # create full distance matrix\n",
    "    N = int((-1+np.sqrt(1+8*len(Z_NN)))/2)\n",
    "    D_NN = np.zeros(shape=(N,N))\n",
    "    D_NN[np.triu_indices(N)] = Z_NN\n",
    "    D_NN = D_NN.T\n",
    "    D_NN[np.triu_indices(N)] = Z_NN\n",
    "    return D_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4d3bd-a97d-4074-a942-cc51d3dd7378",
   "metadata": {},
   "source": [
    "## Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0bcb8d-ef7f-4b88-bc72-66f21a7844d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat = infer_dist_mat(siamese_model, X_train, verbosity)\n",
    "if IsSaveModel:\n",
    "    to_s3_npy(dist_mat, s3_client, bucket_name, os.path.join(s3_model_train_dir_path, 'dist_mat.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873b7b4-3560-4fe4-ac25-bee32514f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_scores = np.mean(dist_mat, axis=1)\n",
    "if IsSaveModel:\n",
    "    to_s3_npy(weird_scores, s3_client, bucket_name, os.path.join(s3_model_train_dir_path, 'weird_scores.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911fdd9-b312-44a2-a967-3a130d1e0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "sne = TSNE(n_components=2, perplexity=25, metric='precomputed', verbose=1, random_state=seed, init='random').fit_transform(dist_mat)\n",
    "if IsSaveModel:\n",
    "    to_s3_npy(sne, s3_client, bucket_name, os.path.join(s3_model_train_dir_path, 'tsne.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df8f35-9f1b-4f21-93be-fcf5625e2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "tmp = plt.hist(weird_scores, bins=60, color=\"g\")\n",
    "plt.title(\"Weirdness score histogram\")\n",
    "plt.ylabel(\"N\")\n",
    "plt.xlabel(\"weirdness score\")\n",
    "if IsSaveModel:\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_model_train_dir_path, 'weirdness_scores_histogram.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6aad7-cf5e-4fb7-80a8-a9ffcb2ed6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = dist_mat[np.tril_indices(dist_mat.shape[0], -1)]\n",
    "\n",
    "fig = plt.figure()\n",
    "tmp = plt.hist(distances, bins=100)\n",
    "plt.title(\"Distances histogram\")\n",
    "plt.ylabel(\"N\")\n",
    "plt.xlabel(\"distance\")\n",
    "\n",
    "if IsSaveModel:\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_model_train_dir_path, 'distances_histogram.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c712428-cb37-44ec-b2ae-67130c4867cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sne_f1 = sne[:, 0]\n",
    "sne_f2 = sne[:, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "im_scat = ax.scatter(sne_f1, sne_f2, s=3, c=weird_scores, cmap=plt.cm.get_cmap('jet'), picker=1)\n",
    "ax.set_xlabel('t-SNE Feature 1')\n",
    "ax.set_ylabel('t-SNE Feature 2')\n",
    "ax.set_title(r't-SNE Scatter Plot Colored by Weirdness score')\n",
    "clb = fig.colorbar(im_scat, ax=ax)\n",
    "clb.ax.set_ylabel('Weirdness', rotation=270)\n",
    "plt.show()\n",
    "\n",
    "if IsSaveModel:\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_model_train_dir_path, 'tsne_colored_by_weirdness.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77861c24-059b-41ef-890c-afd4c416ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = gs_train.snMedian\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "import matplotlib.colors as colors\n",
    "im_scat = ax.scatter(sne_f1, sne_f2, s=3, c=snr, cmap=plt.cm.get_cmap('jet'), norm=colors.LogNorm(vmin=snr.min(), vmax=80))\n",
    "ax.set_xlabel('t-SNE Feature 1')\n",
    "ax.set_ylabel('t-SNE Feature 2')\n",
    "ax.set_title(r't-SNE Scatter Plot Colored by SNR')\n",
    "clb = fig.colorbar(im_scat, ax=ax)\n",
    "clb.ax.set_ylabel('SNR', rotation=270)\n",
    "plt.show()\n",
    "\n",
    "if IsSaveModel:\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_model_train_dir_path, 'tsne_colored_by_snr.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d40ce-b145-4105-938e-c81cd32328ed",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372d09a-11a4-4084-9216-ba8469c0aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat_test = infer_dist_mat(siamese_model, X_test, verbosity)\n",
    "if IsSaveModel:\n",
    "    to_s3_npy(dist_mat_test, s3_client, bucket_name, os.path.join(s3_model_test_dir_path, 'dist_mat.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eea1ab-90c9-47b3-bcf7-5ffb4b69b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_scores_test = np.mean(dist_mat_test, axis=1)\n",
    "if IsSaveModel:\n",
    "    to_s3_npy(weird_scores_test, s3_client, bucket_name, os.path.join(s3_model_test_dir_path, 'weird_scores.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc5823-f0d2-4917-8263-09d55c076495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "sne_test = TSNE(n_components=2, perplexity=25, metric='precomputed', verbose=1, random_state=seed, init='random').fit_transform(dist_mat_test)\n",
    "if IsSaveModel:\n",
    "    to_s3_npy(sne_test, s3_client, bucket_name, os.path.join(s3_model_test_dir_path, 'tsne.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e52d4-0b45-4ea6-8061-b82ca2f24058",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "tmp = plt.hist(weird_scores_test, bins=60, color=\"g\")\n",
    "plt.title(\"Weirdness score histogram\")\n",
    "plt.ylabel(\"N\")\n",
    "plt.xlabel(\"weirdness score\")\n",
    "if IsSaveModel:\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_model_test_dir_path, 'weirdness_scores_histogram.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56492464-c3b6-4540-af9f-c91276926da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_test = dist_mat_test[np.tril_indices(dist_mat_test.shape[0], -1)]\n",
    "\n",
    "fig = plt.figure()\n",
    "tmp = plt.hist(distances_test, bins=100)\n",
    "plt.title(\"Distances histogram\")\n",
    "plt.ylabel(\"N\")\n",
    "plt.xlabel(\"distance\")\n",
    "\n",
    "if IsSaveModel:\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_model_test_dir_path, 'distances_histogram.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc93f89-9987-44f9-9c85-969d966a33b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sne_f1_test = sne_test[:, 0]\n",
    "sne_f2_test = sne_test[:, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "im_scat = ax.scatter(sne_f1_test, sne_f2_test, s=3, c=weird_scores_test, cmap=plt.cm.get_cmap('jet'), picker=1)\n",
    "ax.set_xlabel('t-SNE Feature 1')\n",
    "ax.set_ylabel('t-SNE Feature 2')\n",
    "ax.set_title(r't-SNE Scatter Plot Colored by Weirdness score')\n",
    "clb = fig.colorbar(im_scat, ax=ax)\n",
    "clb.ax.set_ylabel('Weirdness', rotation=270)\n",
    "plt.show()\n",
    "\n",
    "if IsSaveModel:\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_model_test_dir_path, 'tsne_colored_by_weirdness.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69052a30-d450-47dd-9644-e6634dd0f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_test = gs_test.snMedian\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111)\n",
    "import matplotlib.colors as colors\n",
    "im_scat = ax.scatter(sne_f1_test, sne_f2_test, s=3, c=snr_test, cmap=plt.cm.get_cmap('jet'), norm=colors.LogNorm(vmin=snr.min(), vmax=80))\n",
    "ax.set_xlabel('t-SNE Feature 1')\n",
    "ax.set_ylabel('t-SNE Feature 2')\n",
    "ax.set_title(r't-SNE Scatter Plot Colored by SNR')\n",
    "clb = fig.colorbar(im_scat, ax=ax)\n",
    "clb.ax.set_ylabel('SNR', rotation=270)\n",
    "plt.show()\n",
    "\n",
    "if IsSaveModel:\n",
    "    to_s3_fig(fig, s3_client, bucket_name, os.path.join(s3_model_test_dir_path, 'tsne_colored_by_snr.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
